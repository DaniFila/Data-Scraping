{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically install the necessary libraries\n",
    "!pip install pandas pyarrow google-cloud-storage google-cloud-bigquery yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "try:\n",
    "    import datetime as dt\n",
    "    import pytz\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "    from google.cloud import storage\n",
    "    import time\n",
    "    from google.cloud import bigquery\n",
    "    import yfinance as yf\n",
    "\n",
    "    print(\"Libraries imported correctly.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing libraries: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_parquet_from_gcs(bucket_name, source_blob_name, destination_file_name,credential_path):\n",
    "    \"\"\"\n",
    "    Downloads a Parquet file from Google Cloud Storage.\n",
    "\n",
    "    Args:\n",
    "        bucket_name (str): The name of the GCS bucket.\n",
    "        source_blob_name (str): The path of the file in the bucket.\n",
    "        destination_file_name (str): The local path where the file will be saved.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the Google Cloud Storage client\n",
    "    storage_client = storage.Client.from_service_account_json(credential_path)\n",
    "\n",
    "    # Get the bucket and the blob (file)\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "\n",
    "    # Download the file to the local path\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "    print(f\"File {source_blob_name} successfully downloaded to {destination_file_name}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"parquet-dataset-financial-data\"\n",
    "source_blob_name = \"main_financial_data.parquet\"\n",
    "destination_file_name = \"/Users/danifila/Downloads/main_financial_data.parquet\"\n",
    "credential_path = \"/Users/danifila/Desktop/UpWork/dani-financial-1ca621e0a4c6.json\"\n",
    "\n",
    "download_parquet_from_gcs(bucket_name,source_blob_name,destination_file_name,credential_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_yahoo_finance_data(isin,symbol_isin_dict):\n",
    "    \"\"\"\n",
    "    Downloads historical data and summary information from Yahoo Finance for a given ISIN.\n",
    "    \n",
    "    Args:\n",
    "        isin (str): The ISIN of the financial instrument.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Use the ISIN to create a Ticker object\n",
    "        ticker = yf.Ticker(isin)\n",
    "        \n",
    "        # Download historical data\n",
    "        historical_data = ticker.history(period=\"max\",actions=True)  # Adjust the period if needed\n",
    "        \n",
    "        # Download summary data\n",
    "        summary = ticker.info\n",
    "        \n",
    "        # Extract analyst price targets and market cap if available\n",
    "        analyst_targets = summary.get('targetMeanPrice', np.nan)\n",
    "        market_cap = summary.get('marketCap', np.nan)\n",
    "        \n",
    "         # Add ISIN, scraping_time_stamp, and url_scraping columns\n",
    "        scraping_time_stamp = dt.datetime.now() # Get the current timestamp\n",
    "        url_scraping = f\"https://finance.yahoo.com/quote/{isin}\"  # Construct the URL for scraping\n",
    "\n",
    "\n",
    "        # Add ISIN as a column\n",
    "        historical_data['ISIN'] = isin\n",
    "        historical_data['scraping_time_stamp'] = scraping_time_stamp\n",
    "        historical_data[\"url_scraping\"] = url_scraping\n",
    "\n",
    "        \n",
    "        if historical_data.index.name == 'Date':\n",
    "            # Convert index to a column and reset the index\n",
    "            historical_data = historical_data.reset_index()\n",
    "            # Ensure 'Date' column is datetime and remove timezone\n",
    "            historical_data['Date'] = pd.to_datetime(historical_data['Date'])\n",
    "\n",
    "            \n",
    "        # Add ISIN as the first column\n",
    "        historical_data['ISIN'] = isin\n",
    "        historical_data = historical_data[['ISIN'] + [col for col in historical_data.columns if col not in ['ISIN']]]\n",
    "\n",
    "\n",
    "\n",
    "        return {\n",
    "            'historical_data': historical_data,\n",
    "            'summary': { \n",
    "                'ISIN':isin,\n",
    "                'analyst_price_targets': analyst_targets,\n",
    "                'market_cap': market_cap,\n",
    "                'scraping_time_stamp': scraping_time_stamp,\n",
    "                'url_scraping': url_scraping\n",
    "            }\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading data for ISIN {isin}: {e}\")\n",
    "        # Try using the Symbol if the ISIN fails\n",
    "        symbol = symbol_isin_dict.get(isin)\n",
    "        if symbol:\n",
    "            print(f\"Trying again with Symbol: {symbol}\")\n",
    "            try:\n",
    "                # Use the Symbol to create a Ticker object\n",
    "                ticker = yf.Ticker(symbol)\n",
    "                \n",
    "                # Download historical data\n",
    "                historical_data = ticker.history(period=\"max\", actions=True)\n",
    "                \n",
    "                # Download summary data\n",
    "                summary = ticker.info\n",
    "\n",
    "                # Extract analyst price targets and market cap if available\n",
    "                analyst_targets = summary.get('targetMeanPrice', np.nan)\n",
    "                market_cap = summary.get('marketCap', np.nan)\n",
    "\n",
    "                # Add ISIN, scraping_time_stamp, and url_scraping columns\n",
    "                scraping_time_stamp = dt.datetime.now()\n",
    "                url_scraping = f\"https://finance.yahoo.com/quote/{symbol}\"\n",
    "\n",
    "                historical_data['ISIN'] = isin\n",
    "                historical_data['scraping_time_stamp'] = scraping_time_stamp\n",
    "                historical_data[\"url_scraping\"] = url_scraping\n",
    "\n",
    "                if historical_data.index.name == 'Date':\n",
    "                    historical_data = historical_data.reset_index()\n",
    "                    historical_data['Date'] = pd.to_datetime(historical_data['Date'])\n",
    "\n",
    "                historical_data['ISIN'] = isin\n",
    "                historical_data = historical_data[['ISIN'] + [col for col in historical_data.columns if col not in ['ISIN']]]\n",
    "\n",
    "\n",
    "\n",
    "                return {\n",
    "                    'historical_data': historical_data,\n",
    "                    'summary': {\n",
    "                        'ISIN': isin,\n",
    "                        'analyst_price_targets': analyst_targets,\n",
    "                        'market_cap': market_cap,\n",
    "                        'scraping_time_stamp': scraping_time_stamp,\n",
    "                        'url_scraping': url_scraping\n",
    "                    }\n",
    "                }\n",
    "\n",
    "            except Exception as e2:\n",
    "                print(f\"Error downloading data with Symbol {symbol}: {e2}\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "def isin_to_dict_from_parquet(destination_file_name):\n",
    "    parquet = pd.read_parquet(destination_file_name)\n",
    "\n",
    "    parquet = parquet.drop_duplicates(subset=\"ISIN\")\n",
    "    parquet_selected = parquet[[\"ISIN\",\"Symbol\"]]\n",
    "    symbol_isin_dict = parquet_selected.set_index(\"ISIN\")[\"Symbol\"].to_dict()\n",
    "\n",
    "    return symbol_isin_dict\n",
    "\n",
    "\n",
    "def save_to_parquet(data, filename):\n",
    "    \"\"\"\n",
    "    Save the data to a Parquet file, ensuring all columns have appropriate types.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): The DataFrame to be saved.\n",
    "        filename (str): The output Parquet file name.\n",
    "    \"\"\"\n",
    "    if data is not None:\n",
    "        # Check if 'Adj Close' column exists and convert it to float\n",
    "        if 'Adj Close' in data.columns:\n",
    "            data['Adj Close'] = pd.to_numeric(data['Adj Close'], errors='coerce')  # Coerce to float, replace errors with NaN\n",
    "            data['Adj Close'] = data['Adj Close'].fillna(0)  # Optional: Replace NaNs with 0 or another value\n",
    "        \n",
    "        # Remove 'Dividends' and 'stock_split' columns if they exist\n",
    "        if 'Dividends' in data.columns:\n",
    "            data = data.drop(columns=['Dividends'])\n",
    "        if 'Stock Splits' in data.columns:\n",
    "            data = data.drop(columns=['Stock Splits'])\n",
    "\n",
    "        if 'Date' in data.columns:\n",
    "            data['Date'] = pd.to_datetime(data[\"Date\"]).dt.tz_localize(None).astype('datetime64[us]')\n",
    "\n",
    "        if 'scraping_time_stamp' in data.columns:\n",
    "            data['scraping_time_stamp'] = pd.to_datetime(data['scraping_time_stamp']).dt.tz_localize(None).astype('datetime64[us]')\n",
    "    \n",
    "        # Convert DataFrame to PyArrow Table\n",
    "        table = pa.Table.from_pandas(data)\n",
    "\n",
    "        # Write the table to Parquet file\n",
    "        pq.write_table(table, filename)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "    else:\n",
    "        print(f\"No data to save for {filename}\")\n",
    "\n",
    "\n",
    "def process_isins(symbol_isin_dict):\n",
    "    \"\"\"\n",
    "    Processes a list of ISINs by downloading data from Yahoo Finance for each and saving it locally.\n",
    "    \n",
    "    \"\"\"\n",
    "    for i, isin in enumerate(symbol_isin_dict):\n",
    "        print(isin)\n",
    "        print(f\"Processing ISIN: {isin}\")\n",
    "        data = download_yahoo_finance_data(isin,symbol_isin_dict)\n",
    "        \n",
    "        if data:\n",
    "            historical_data = data.get('historical_data')\n",
    "            summary = data.get('summary')\n",
    "            \n",
    "            # Save historical data to a Parquet file\n",
    "            historical_filename = f\"{isin}_historical_data.parquet\"\n",
    "            save_to_parquet(historical_data, historical_filename)\n",
    "            \n",
    "            # Save summary data to a Parquet file\n",
    "            summary_filename = f\"{isin}_summary_data.parquet\"\n",
    "            save_to_parquet(pd.DataFrame([summary]), summary_filename)\n",
    "            \n",
    "        # Wait for 60 seconds after every 200 ISINs\n",
    "        if (i + 1) % 200 == 0:\n",
    "            print(\"Waiting for 60 seconds to avoid rate limiting...\")\n",
    "            time.sleep(60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_isin_dict = isin_to_dict_from_parquet(destination_file_name)\n",
    "process_isins(symbol_isin_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_gcs(bucket_name,source_file_name,destination_blob_name,credential_path):\n",
    "    \"\"\"\n",
    "    Uploads a local Parquet file to a GCS bucket.\n",
    "\n",
    "    :param bucket_name: The name of the GCS bucket.\n",
    "    :param source_file_name: The local path to the Parquet file to be uploaded.\n",
    "    :param destination_blob_name: The name for the file in the GCS bucket.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a GCS client\n",
    "        storage_client = storage.Client.from_service_account_json(credential_path)\n",
    "\n",
    "        # Get the GCS bucket\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "        # Create a blob (an object in the GCS bucket) and upload the file\n",
    "        blob = bucket.blob(destination_blob_name)\n",
    "        blob.upload_from_filename(source_file_name)\n",
    "\n",
    "        print(f\"File {source_file_name} successfully uploaded as {destination_blob_name} to bucket {bucket_name}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading to GCS: {e}\")\n",
    "\n",
    "\n",
    "def upload_isin_files_to_gcs(bucket_name, symbol_isin_dict,credential_path):\n",
    "    \"\"\"\n",
    "    Uploads the Parquet files related to ISINs to the GCS bucket, organizing them into folders by ISIN.\n",
    "    \"\"\"\n",
    "    for isin in symbol_isin_dict:\n",
    "        historical_filename = f\"{isin}_historical_data.parquet\"\n",
    "        summary_filename = f\"{isin}_summary_data.parquet\"\n",
    "        \n",
    "        # Upload the files to the GCS bucket\n",
    "        upload_to_gcs(bucket_name, historical_filename, f\"{isin}/{historical_filename}\",credential_path)\n",
    "        upload_to_gcs(bucket_name, summary_filename, f\"{isin}/{summary_filename}\",credential_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_isin_files_to_gcs(bucket_name,symbol_isin_dict,credential_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parquet_files_to_bigquery(symbol_isin_dict, table_historical_id, table_summary_id,credential_path):\n",
    "    \"\"\"\n",
    "    Loads Parquet files for each ISIN from GCS into BigQuery tables.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name (str): The name of the GCS bucket.\n",
    "        isin_list (list): A list of ISINs to process.\n",
    "        project_id (str): Your Google Cloud project ID.\n",
    "        table_historical_id (str): The BigQuery table ID for historical data (e.g., 'project.dataset.table_historical').\n",
    "        table_summary_id (str): The BigQuery table ID for summary data (e.g., 'project.dataset.table_summary').\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize BigQuery and Storage clients\n",
    "    bigquery_client = bigquery.Client.from_service_account_json(credential_path)\n",
    "    \n",
    "    for isin in symbol_isin_dict:\n",
    "        try:\n",
    "            # Define GCS paths for historical and summary parquet files for the current ISIN\n",
    "            historical_uri = f\"gs://parquet-dataset-financial-data/{isin}/{isin}_historical_data.parquet\"\n",
    "            summary_uri = f\"gs://parquet-dataset-financial-data/{isin}/{isin}_summary_data.parquet\"\n",
    "            \n",
    "            # Set up the job configuration for loading historical data\n",
    "            job_config_historical = bigquery.LoadJobConfig(\n",
    "                source_format=bigquery.SourceFormat.PARQUET,\n",
    "                write_disposition=bigquery.WriteDisposition.WRITE_APPEND  # Append data to the table\n",
    "            )\n",
    "            \n",
    "            # Load historical data from GCS to BigQuery\n",
    "            load_job_historical = bigquery_client.load_table_from_uri(\n",
    "                historical_uri,\n",
    "                table_historical_id,\n",
    "                job_config=job_config_historical\n",
    "            )\n",
    "            \n",
    "            print(f\"Starting load job for historical data ISIN: {isin}\")\n",
    "            load_job_historical.result()  # Wait for the job to complete\n",
    "            print(f\"Loaded historical data for ISIN: {isin} into {table_historical_id}\")\n",
    "            \n",
    "            # Set up the job configuration for loading summary data\n",
    "            job_config_summary = bigquery.LoadJobConfig(\n",
    "                source_format=bigquery.SourceFormat.PARQUET,\n",
    "                write_disposition=bigquery.WriteDisposition.WRITE_APPEND, # Append data to the table\n",
    "            )\n",
    "            \n",
    "            # Load summary data from GCS to BigQuery\n",
    "            load_job_summary = bigquery_client.load_table_from_uri(\n",
    "                summary_uri,\n",
    "                table_summary_id,\n",
    "                job_config=job_config_summary\n",
    "            )\n",
    "            \n",
    "            print(f\"Starting load job for summary data ISIN: {isin}\")\n",
    "            load_job_summary.result()  # Wait for the job to complete\n",
    "            print(f\"Loaded summary data for ISIN: {isin} into {table_summary_id}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing ISIN {isin}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(\"Data load for all ISINs completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_historical_id = \"dani-financial.financial_data.historical_price\"\n",
    "table_summary_id = \"dani-financial.financial_data.summary_data\"\n",
    "load_parquet_files_to_bigquery(symbol_isin_dict, table_historical_id, table_summary_id,credential_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
