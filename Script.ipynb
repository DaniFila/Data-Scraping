{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 7692,
          "status": "ok",
          "timestamp": 1725793596321,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "5KMYTMMKTz9l",
        "outputId": "e2b051e3-321d-4a38-ec4f-df19fb8b7f1b"
      },
      "outputs": [],
      "source": [
        "# Automatically install the necessary libraries\n",
        "!pip install pandas selenium pyarrow google-cloud-storage google-cloud-bigquery\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 2019,
          "status": "ok",
          "timestamp": 1725793598336,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "FFcTe4HGTz9m",
        "outputId": "72bbb567-5515-4308-ecd4-aa696cbf2b7d"
      },
      "outputs": [],
      "source": [
        "# Importing the necessary libraries\n",
        "try:\n",
        "    import pandas as pd\n",
        "    from selenium import webdriver\n",
        "    from selenium.webdriver.chrome.options import Options\n",
        "    from selenium.webdriver.common.by import By\n",
        "    import os\n",
        "    import pyarrow as pa\n",
        "    import pyarrow.parquet as pq\n",
        "    import time\n",
        "    from datetime import datetime\n",
        "    from google.cloud import storage\n",
        "    from google.cloud import bigquery\n",
        "    from google.oauth2 import service_account\n",
        "\n",
        "    print(\"Libraries imported correctly.\")\n",
        "except ImportError as e:\n",
        "    print(f\"Error importing libraries: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFU_9UjkTz9m"
      },
      "outputs": [],
      "source": [
        "# Function to configure the WebDriver\n",
        "def setup_driver():\n",
        "    \"\"\"\n",
        "    Configures and returns an instance of Chrome WebDriver in headless mode.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Configures Chrome options for headless mode\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
        "        chrome_options.add_argument(\"--disable-gpu\")  # Disable GPU hardware acceleration (optional)\n",
        "        chrome_options.add_argument(\"--no-sandbox\")  # Disable sandboxing (optional)\n",
        "        prefs = {\n",
        "            \"download.prompt_for_download\": False,\n",
        "            \"download.directory_upgrade\": True,\n",
        "            \"safebrowsing.enabled\": True\n",
        "        }\n",
        "        chrome_options.add_experimental_option(\"prefs\", prefs)\n",
        "\n",
        "        # Create and return the driver with the configured options\n",
        "        driver = webdriver.Chrome(options=chrome_options)\n",
        "        print(\"Driver configured in headless mode.\")\n",
        "        return driver\n",
        "    except Exception as e:\n",
        "        print(f\"Error in driver configuration: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52CN2j0tTz9m"
      },
      "outputs": [],
      "source": [
        "def get_csv_data(driver):\n",
        "    \"\"\"\n",
        "    Download the CSV file from the web page and save it as a Parquet file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Visit the page\n",
        "        driver.get(\"https://live.euronext.com/en/markets/milan/equities/list\")\n",
        "        time.sleep(5)  # Wait for the page to load\n",
        "\n",
        "        # accept the cookie\n",
        "        try:\n",
        "            cookie = driver.find_element(By.ID, \"onetrust-accept-btn-handler\")\n",
        "            cookie.click()\n",
        "            print(\"Cookie accepted.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error accepting cookies: {e}\")\n",
        "            return\n",
        "\n",
        "        # Find and click the button to download the file\n",
        "        try:\n",
        "            download = driver.find_element(By.XPATH, \"/html/body/div[2]/div[1]/div/div/div[1]/div[3]/div/main/section/div[3]/div/div/div[1]/div[2]/div[1]/div[2]/button\")\n",
        "            download.click()\n",
        "            time.sleep(2)  # Wait for the download menu to open\n",
        "            print(\"Download button clicked.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error clicking the download button: {e}\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            # Select CSV as the download format\n",
        "            down_csv = driver.find_element(By.XPATH, \"/html/body/div[7]/div/div/div[2]/fieldset[1]/div[2]/label\")\n",
        "            down_csv.click()\n",
        "            print(\"CSV format selected.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error selecting the CSV format:: {e}\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            # Click to start the download\n",
        "            down_file = driver.find_element(By.XPATH, \"/html/body/div[7]/div/div/div[2]/input\")\n",
        "            down_file.click()\n",
        "            print(\"Download started.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error starting the download: {e}\")\n",
        "            return\n",
        "\n",
        "        # Wait for the file to download\n",
        "        time.sleep(5)\n",
        "\n",
        "        # Determine the download folder based on the operating system\n",
        "        if os.name == 'nt':  # Windows\n",
        "            download_dir = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\n",
        "        elif os.name == 'posix':  # macOS o Linux\n",
        "            download_dir = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\n",
        "\n",
        "        # Search for the most recent file in the download folder\n",
        "        try:\n",
        "            downloaded_files = os.listdir(download_dir)\n",
        "            downloaded_files = [os.path.join(download_dir, f) for f in downloaded_files]\n",
        "            latest_file = max(downloaded_files, key=os.path.getctime)  # Trova il file pi√π recente\n",
        "            print(f\"Downloaded file found: {latest_file}\")\n",
        "            return latest_file\n",
        "        except Exception as e:\n",
        "            print(f\"Error searching for the downloaded file: {e}\")\n",
        "            return\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"General error during file download: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcoAvTssTz9n"
      },
      "outputs": [],
      "source": [
        "def csv_to_df(latest_file):\n",
        "    try:\n",
        "        # Check if the file is a CSV and load it into a DataFrame\n",
        "        df = pd.read_csv(latest_file, sep=\";\",skiprows=[1,2,3])\n",
        "        print(f\"CSV file loaded successfully: {latest_file}\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading the CSV file: {e}\")\n",
        "\n",
        "def clean_numeric_columns(df):\n",
        "    # List of columns that might contain apostrophes\n",
        "    columns = [\"Open Price\", \"High Price\", \"low Price\", \"last Price\",\n",
        "               \"last Trade MIC Time\", \"Volume\", \"Turnover\", \"Closing Price\", \"Closing Price DateTime\"]\n",
        "    \n",
        "    # Loop through each column in the list\n",
        "    for column in columns:\n",
        "        # Replace apostrophes with an empty string using regex\n",
        "        df[column] = df[column].replace(\"'\", \"\", regex=True)\n",
        "    \n",
        "    return df\n",
        "\n",
        "def convert_numeric_col(df):\n",
        "    colum = [\"Open Price\",\"High Price\",\"low Price\",\"last Price\",\"Turnover\",\"Closing Price\"]\n",
        "    for col in colum:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    df[\"Volume\"] = pd.to_numeric(df[\"Volume\"], errors='coerce')\n",
        "    df['Volume'] = df['Volume'].fillna(0).astype(int)\n",
        "    return df\n",
        "\n",
        "def convert_datetime_col(df):    \n",
        "    df[\"last Trade MIC Time\"] = pd.to_datetime(df[\"last Trade MIC Time\"],format='%d/%m/%Y %H:%M', errors='coerce')\n",
        "    df['last Trade MIC Time'] = df['last Trade MIC Time'].values.astype('datetime64[us]')\n",
        "    df['Closing Price DateTime'] = pd.to_datetime(df['Closing Price DateTime'], format='%d/%m/%Y', errors='coerce')\n",
        "    df['Closing Price DateTime'] = df['Closing Price DateTime'].values.astype('datetime64[us]')\n",
        "    return df\n",
        "\n",
        "def fix_col(df):\n",
        "    try:\n",
        "        clean_numeric_columns(df)\n",
        "        convert_numeric_col(df)\n",
        "        convert_datetime_col(df)\n",
        "        print(\"Columns Fixed\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error during fixing col: {e}\")\n",
        "        return None\n",
        "    \n",
        "    \n",
        "def add_scarping_detail_and_convert_df_to_parquet(df):\n",
        "    \"\"\"\n",
        "    Adds metadata columns to the DataFrame.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df['scraping_timestamp'] = datetime.now()\n",
        "        df['scraping_url'] = \"https://live.euronext.com/en/markets/milan/equities/list\"\n",
        "        print(\"Metadata columns added.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error adding metadata columns: {e}\")\n",
        "    try:\n",
        "        # Determine the download folder based on the operating system\n",
        "        if os.name == 'nt':  # Windows\n",
        "            download_dir = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\n",
        "        elif os.name == 'posix':  # macOS o Linux\n",
        "            download_dir = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\n",
        "\n",
        "\n",
        "        # Convert the DataFrame to Parquet format\n",
        "        table = pa.Table.from_pandas(df)\n",
        "        parquet_file = os.path.join(download_dir, 'financial_data.parquet')\n",
        "        pq.write_table(table, parquet_file)\n",
        "        print(f\"Parquet file saved as: {parquet_file}\")\n",
        "    except:\n",
        "        print(\"Error during convert the file to parquet\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 23903,
          "status": "ok",
          "timestamp": 1725793622234,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "BuX0cZVPTz9n",
        "outputId": "e29187b5-06ff-4b9f-dbda-24e5a7cf714d"
      },
      "outputs": [],
      "source": [
        "# Execute the function\n",
        "\n",
        "# Configure and get the driver\n",
        "driver = setup_driver()\n",
        "\n",
        "# Run the function to download and get csv\n",
        "latest_file = get_csv_data(driver)\n",
        "\n",
        "# save df file to parquet with scraping metadata\n",
        "df = csv_to_df(latest_file)\n",
        "df = fix_col(df)\n",
        "add_scarping_detail_and_convert_df_to_parquet(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOlKhTB3Tz9n"
      },
      "outputs": [],
      "source": [
        "def upload_to_gcs(bucket_name,source_file_name,destination_blob_name,credential_path):\n",
        "    \"\"\"\n",
        "    Uploads a local Parquet file to a GCS bucket.\n",
        "\n",
        "    :param bucket_name: The name of the GCS bucket.\n",
        "    :param source_file_name: The local path to the Parquet file to be uploaded.\n",
        "    :param destination_blob_name: The name for the file in the GCS bucket.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create a GCS client\n",
        "        storage_client = storage.Client.from_service_account_json(credential_path)\n",
        "\n",
        "        # Get the GCS bucket\n",
        "        bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "        # Create a blob (an object in the GCS bucket) and upload the file\n",
        "        blob = bucket.blob(destination_blob_name)\n",
        "        blob.upload_from_filename(source_file_name)\n",
        "\n",
        "        print(f\"File {source_file_name} successfully uploaded as {destination_blob_name} to bucket {bucket_name}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error uploading to GCS: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 10,
          "status": "ok",
          "timestamp": 1725793622234,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "6apfwYDiTz9n",
        "outputId": "e7affefb-a725-4a33-ce96-5b8f9d257940"
      },
      "outputs": [],
      "source": [
        "bucket_name = 'parquet-dataset-financial-data'  # GCS bucket name\n",
        "source_file_name = '/Users/danifila/Downloads/financial_data.parquet'  # Local path to the Parquet file\n",
        "destination_blob_name = 'main_financial_data.parquet'  # Desired name for the file in the GCS bucket\n",
        "credentials_path = '/Users/danifila/Desktop/UpWork/dani-financial-1ca621e0a4c6.json'  # Path to the JSON credentials file\n",
        "\n",
        "\n",
        "upload_to_gcs(bucket_name,source_file_name,destination_blob_name,credentials_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOMdthC8Tz9n"
      },
      "outputs": [],
      "source": [
        "def load_parquet_to_bigquery(dataset_id, table_id, gcs_uri, credentials_path):\n",
        "    \"\"\"\n",
        "    Loads a Parquet file from GCS to a table in BigQuery.\n",
        "\n",
        "    :param dataset_id: ID of the BigQuery dataset.\n",
        "    :param table_id: ID of the BigQuery table.\n",
        "    :param gcs_uri: URI of the Parquet file in GCS (e.g., gs://bucket-name/file-name.parquet).\n",
        "    :param credentials_path: Path to the JSON credentials file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create a BigQuery client with specific credentials\n",
        "        client = bigquery.Client.from_service_account_json(credentials_path)\n",
        "\n",
        "        # Configure the load job\n",
        "        job_config = bigquery.LoadJobConfig(\n",
        "            source_format=bigquery.SourceFormat.PARQUET,  # Specify the file format\n",
        "            write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE # Overwrite the existing table\n",
        "        )\n",
        "\n",
        "        # Define the reference to the BigQuery table\n",
        "        table_ref = f\"{dataset_id}.{table_id}\"\n",
        "\n",
        "        # Load data from GCS to BigQuery\n",
        "        load_job = client.load_table_from_uri(\n",
        "            gcs_uri,  # URI of the Parquet file in GCS\n",
        "            table_ref,  # Reference to the BigQuery table\n",
        "            job_config=job_config\n",
        "        )\n",
        "\n",
        "        # Wait for the job to complete\n",
        "        load_job.result()\n",
        "\n",
        "        print(f\"Loaded {load_job.output_rows} rows into table {table_ref}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during loading to BigQuery: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 8,
          "status": "ok",
          "timestamp": 1725793622234,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "qTqHCKmRTz9o",
        "outputId": "0bf6f0b1-3876-44c4-f0ed-7c592920fae8"
      },
      "outputs": [],
      "source": [
        "dataset_id = 'financial_data'  # ID of the BigQuery dataset\n",
        "table_id = 'stock_prices'  # ID of the BigQuery table\n",
        "gcs_uri = 'gs://parquet-dataset-financial-data/main_financial_data.parquet'  # URI of the Parquet file in GCS\n",
        "credentials_path = '/Users/danifila/Desktop/UpWork/dani-financial-1ca621e0a4c6.json'  # Path to the JSON credentials file\n",
        "\n",
        "load_parquet_to_bigquery(dataset_id, table_id, gcs_uri, credentials_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Script.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
